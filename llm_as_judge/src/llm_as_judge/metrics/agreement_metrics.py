"""Agreement-based metrics for LLM-as-Judge evaluation."""

import numpy as np
from sklearn.metrics import cohen_kappa_score

def exact_match_agreement(llm_scores, human_scores):
    """
    Calculate the exact match agreement between LLM and human scores.
    
    Args:
        llm_scores (array-like): Scores generated by the LLM
        human_scores (array-like): Scores provided by human evaluators
        
    Returns:
        float: Proportion of exact matches
    """
    return np.mean(np.array(llm_scores) == np.array(human_scores))

def cohen_kappa(llm_scores, human_scores, n_bins=5):
    """
    Calculate Cohen's Kappa score for inter-rater reliability.
    
    Args:
        llm_scores (array-like): Scores generated by the LLM
        human_scores (array-like): Scores provided by human evaluators
        n_bins (int): Number of bins for discretizing scores
        
    Returns:
        float: Cohen's Kappa score
    """
    llm_binned = np.digitize(llm_scores, bins=np.linspace(0, 1, n_bins + 1))
    human_binned = np.digitize(human_scores, bins=np.linspace(0, 1, n_bins + 1))
    return cohen_kappa_score(llm_binned, human_binned) 