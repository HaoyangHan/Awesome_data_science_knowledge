\documentclass{article}
\usepackage{amsmath}

\title{Introduction to Hyperparameters in Large Language Models}
\author{}
\date{}

\begin{document}

\maketitle

When working with large language models (LLMs) like OpenAI's GPT, Google’s PaLM, Anthropic’s Claude, or DeepMind's Gemini, understanding how \textbf{hyperparameters} affect model behavior is crucial for fine-tuning outputs. Among the various hyperparameters, \textbf{temperature} is one of the most significant as it controls the \textbf{randomness} or \textbf{creativity} of the model's responses.

This document explains key hyperparameters used in LLMs, including \textbf{temperature}, \textbf{top-k}, \textbf{top-p}, and \textbf{penalties} (frequency and presence). We'll provide a mathematical foundation for each parameter and discuss what their values mean in practice.

\section{1. Temperature (T)}

Temperature is a hyperparameter that controls the \textbf{randomness} in the model's predictions. A higher temperature makes the model’s responses more random, while a lower temperature makes the output more deterministic and focused.

\subsection*{Mathematical Formula for Adjusting Logits:}
\[
\text{Logits}_{\text{adjusted}} = \frac{\text{Logits}}{T}
\]

\subsection*{Softmax with Temperature:}
\[
P(w_i) = \frac{e^{\frac{\text{Logits}_i}{T}}}{\sum_{j} e^{\frac{\text{Logits}_j}{T}}}
\]

\subsection*{What Does Temperature Mean?}
\begin{itemize}
    \item \textbf{Low Temperature (T < 1.0)}: The model’s output becomes more \textbf{deterministic}, focusing on the most likely predictions. It's ideal for tasks requiring factual, less creative responses.
    \item \textbf{High Temperature (T > 1.0)}: The output becomes more \textbf{random} and creative, producing diverse but potentially less coherent text. This is useful for tasks like brainstorming or creative writing.
    \item \textbf{T = 1.0}: The default setting, offering a balance between creativity and coherence.
\end{itemize}

\section{2. Top-k Sampling}

Top-k sampling restricts the possible next tokens to the top \textbf{k} most likely candidates. This helps in focusing the output on a smaller, more probable set of options, avoiding excessive randomness.

\subsection*{Formula for Top-k Sampling:}
\[
P(w_i) = \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-}k} e^{\text{Logits}_j}}
\]

This ensures the model generates tokens from only the most likely set of words, improving the quality and coherence of the output.

\section{3. Top-p (Nucleus) Sampling}

Top-p sampling, also known as nucleus sampling, works by considering a subset of tokens whose cumulative probability exceeds a specified threshold \(p\).

\subsection*{Formula for Top-p Sampling:}
\[
P(w_i) = \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-p subset}} e^{\text{Logits}_j}}
\]

Top-p sampling dynamically adjusts the number of tokens considered, balancing diversity and quality in a more adaptive way compared to top-k.

\section{4. Frequency Penalty (F)}

The frequency penalty discourages the model from repeating words it has already generated, thus improving the diversity of the generated text.

\subsection*{Formula for Frequency Penalty:}
\[
\text{Logits}_{\text{penalized}} = \text{Logits}_i - F \cdot \text{count}(w_i)
\]

A higher frequency penalty makes the model more likely to avoid repeating words.

\section{5. Presence Penalty (P)}

The presence penalty encourages the model to introduce new words into the generated text, helping avoid over-repetition of the same concepts or tokens.

\subsection*{Formula for Presence Penalty:}
\[
\text{Logits}_{\text{penalized}} = \text{Logits}_i - P \cdot \mathbf{1}[\text{token exists in sequence}]
\]

This penalty ensures the model explores new tokens and ideas, contributing to more varied outputs.

\section{Summary Table: Comparison of Hyperparameters}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Hyperparameter} & \textbf{Formula} & \textbf{Effect} & \textbf{Typical Values \& Meaning} \\
\hline
\textbf{Temperature (T)} & \( \frac{\text{Logits}}{T} \) & Controls randomness. Low values make output deterministic, high values make output creative. & \(T < 1\): deterministic; \(T = 1\): balanced; \(T > 1\): creative \\
\hline
\textbf{Top-k Sampling} & \( \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-}k} e^{\text{Logits}_j}} \) & Restricts the model to the top k most probable tokens. & \(k = 10\) to \(k = 50\), commonly used for more controlled diversity \\
\hline
\textbf{Top-p Sampling} & \( \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-p subset}} e^{\text{Logits}_j}} \) & Ensures diversity by selecting the smallest set of tokens whose cumulative probability is greater than \(p\). & \(p = 0.9\) is common; higher \(p\) increases diversity but may lower coherence \\
\hline
\textbf{Frequency Penalty (F)} & \( \text{Logits}_{\text{penalized}} = \text{Logits}_i - F \cdot \text{count}(w_i) \) & Reduces the probability of repeating tokens. & \(F = 0.5\) to \(F = 2\), higher values discourage repetition \\
\hline
\textbf{Presence Penalty (P)} & \( \text{Logits}_{\text{penalized}} = \text{Logits}_i - P \cdot \mathbf{1}[\text{token exists in sequence}] \) & Encourages new tokens and concepts. & \(P = 0.5\) to \(P = 1\), higher values introduce more variety \\
\hline
\end{tabular}
\end{center}

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Hyperparameter} & \textbf{Formula} & \textbf{Effect} & \textbf{Typical Values \& Meaning} \\
    \hline
    \textbf{Temperature (T)} & \( \text{Logits}_{\text{adjusted}} = \frac{\text{Logits}}{T} \) & Controls randomness. Low values make output deterministic, high values make output creative. & \( T < 1 \): deterministic; \( T = 1 \): balanced; \( T > 1 \): creative \\
    \hline
    \textbf{Top-k Sampling} & \( \text{Logits}_{\text{adjusted}} = \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-}k} e^{\text{Logits}_j}} \) & Restricts the model to the top k most probable tokens. & \( k = 10 \) to \( k = 50 \), commonly used for more controlled diversity \\
    \hline
    \textbf{Top-p Sampling} & \( P(w_i) = \frac{e^{\text{Logits}_i}}{\sum_{j \in \text{Top-p subset}} e^{\text{Logits}_j}} \) & Ensures diversity by selecting the smallest set of tokens whose cumulative probability is greater than \( p \). & \( p = 0.9 \) is common; higher \( p \) increases diversity but may lower coherence \\
    \hline
    \textbf{Frequency Penalty (F)} & \( \text{Logits}_{\text{penalized}} = \text{Logits}_i - F \cdot \text{count}(w_i) \) & Reduces the probability of repeating tokens. & \( F = 0.5 \) to \( F = 2 \), higher values discourage repetition \\
    \hline
    \textbf{Presence Penalty (P)} & \( \text{Logits}_{\text{penalized}} = \text{Logits}_i - P \cdot \mathbf{1}[\text{token exists in sequence}] \) & Encourages new tokens and concepts. & \( P = 0.5 \) to \( P = 1 \), higher values introduce more variety \\
    \hline
    \end{tabular}
    \caption{Comparison of Hyperparameters}
    \end{table}

\section*{Conclusion}

Understanding and tuning these hyperparameters can drastically influence the behavior of language models. \textbf{Temperature} stands out as one of the most important parameters for controlling output creativity versus coherence. By manipulating \textbf{top-k}, \textbf{top-p}, and the penalty parameters, we can further refine and shape the model's behavior to suit specific use cases.

For \textbf{creative tasks}, higher temperatures and a relaxed top-k/top-p setting may be preferred. For \textbf{factual or structured outputs}, lower temperatures and higher penalties on repetition are often better.

By adjusting these hyperparameters, you can fine-tune models to meet the needs of a wide variety of applications, from creative writing to technical documentation.

\end{document}